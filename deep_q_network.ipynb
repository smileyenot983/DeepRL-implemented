{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_q_network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXylxlv2MkNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import random\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f04TiNYVMvhF",
        "colab_type": "code",
        "outputId": "1a7e4e57-6c20-4cae-f254-22db00391171",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "env = gym.make(\"CartPole-v0\").env\n",
        "env.reset()\n",
        "action_space = env.action_space.n\n",
        "state_space = env.observation_space.shape[0]\n",
        "\n",
        "env.close()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWtfquVYM3FV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g0qQ3R9IbuE9",
        "colab": {}
      },
      "source": [
        "#neural network for dqn\n",
        "class Net(nn.Module):\n",
        "    def __init__(self,state_size,action_size):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(state_size,256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256,128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128,64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64,action_size)\n",
        "\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        output = self.layers(x)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeT7Hk8ii1G-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GAMMA = 0.99\n",
        "BUFFER_SIZE = 100000\n",
        "class DQNAgent():\n",
        "    def __init__(self, state_size,action_size,batch_size,GAMMA):\n",
        "        self.GAMMA = GAMMA\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.batch_size = batch_size\n",
        "        #neural network(only one NN for now)\n",
        "        self.q_net = Net(state_size,action_size)\n",
        "        self.train_flag=False    \n",
        "\n",
        "        self.optimizer = optim.Adam(self.q_net.parameters(),lr=0.001)\n",
        "        self.memory = ReplayBuffer(10000)\n",
        "\n",
        "    #writes SARS' to memory and makes backprop\n",
        "    def step(self,state,action,reward,next_state,done):\n",
        "        self.memory.push(state,action,reward,next_state,done)\n",
        "\n",
        "        if self.memory.size()>self.batch_size:\n",
        "            experience = self.memory.sample(self.batch_size)\n",
        "            loss = self.learn(experience)\n",
        "            self.train_flag=True\n",
        "            return loss\n",
        "\n",
        "    #returns action for given state\n",
        "    def act(self,state,epsilon):\n",
        "        # state = torch.tensor(state).float()\n",
        "        state = torch.FloatTensor(state)\n",
        "        # self.q_net.eval()\n",
        "        # with torch.no_grad():\n",
        "        #     action_values = self.q_net(state)\n",
        "        # self.q_net.train()\n",
        "        action_values = self.q_net(state)\n",
        "        if random.random()>epsilon:\n",
        "            return(np.argmax(action_values.detach().numpy()))\n",
        "        else:\n",
        "            return np.random.choice(range(self.action_size))\n",
        "\n",
        "    #backprop\n",
        "    def learn(self,experiences):\n",
        "        GAMMA = 0.99\n",
        "\n",
        "        #experiences - batch from memory SARS'\n",
        "        states,actions,rewards,next_states,dones = experiences\n",
        "\n",
        "        #converting to tensors to work with pytorch\n",
        "        states = torch.FloatTensor(states)\n",
        "        next_states = torch.FloatTensor(next_states)\n",
        "        dones = torch.FloatTensor(dones)\n",
        "        rewards = torch.FloatTensor(rewards)\n",
        "\n",
        "        #loss function is MSE\n",
        "        criterion = nn.MSELoss()\n",
        "        #training mod for neural network\n",
        "        self.q_net.train()\n",
        "\n",
        "        #predicted q_values for batch of states and actions\n",
        "        q_vals=[]\n",
        "        for state,action in zip(states,actions):\n",
        "            q_vals.append(self.q_net(state)[action])\n",
        "        q_vals = torch.stack(q_vals)\n",
        "\n",
        "        #predicted q_values for next_states\n",
        "        next_q_vals = torch.max(self.q_net(next_states),1)[0]\n",
        "\n",
        "        targets = rewards + self.GAMMA * (next_q_vals*(1-dones))\n",
        "\n",
        "        #loss = (q_values - target_q_values)**2\n",
        "        loss = criterion(q_vals,targets)\n",
        "        \n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffcO0CSP58LN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer:\n",
        "\n",
        "    def __init__(self,max_size):\n",
        "        self.max_size = max_size\n",
        "        self.buffer = []\n",
        "        self.last_exp=0\n",
        "\n",
        "    def push(self,state,action,reward,next_state,done):\n",
        "        experience = (state,action,reward,next_state,done)\n",
        "        if self.last_exp>self.max_size:\n",
        "            index = self.last_exp%self.max_size\n",
        "            self.buffer[index] = experience\n",
        "        else:\n",
        "            self.buffer.append(experience)\n",
        "        self.last_exp+=1\n",
        "\n",
        "    def sample(self,batch_size):\n",
        "        state_batch=[]\n",
        "        action_batch=[]\n",
        "        reward_batch=[]\n",
        "        next_state_batch=[]\n",
        "        done_batch=[]\n",
        "\n",
        "        batch = random.sample(self.buffer,batch_size)\n",
        "\n",
        "        for experience in batch:\n",
        "            state,action,reward,next_state,done = experience\n",
        "            \n",
        "            state_batch.append(state)\n",
        "            action_batch.append(action)\n",
        "            reward_batch.append(reward)\n",
        "            next_state_batch.append(next_state)\n",
        "            done_batch.append(done)\n",
        "\n",
        "        return (state_batch,action_batch,reward_batch,next_state_batch,done_batch)\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.buffer)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBafX0CbFLMj",
        "colab_type": "code",
        "outputId": "c9e7f3f7-0e39-4257-9ba1-7ae5bc12e2ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        }
      },
      "source": [
        "agent = DQNAgent(4,2,batch_size=32,GAMMA = 0.95)\n",
        "\n",
        "def training(n_episodes=200, max_t = 2000,eps_start = 1.0,eps_end=0.01,eps_decay=0.996):\n",
        "    scores=[]\n",
        "    losses=[]\n",
        "    epsilon = eps_start\n",
        "\n",
        "    for epoch in range(max_t):\n",
        "        state = env.reset()\n",
        "        epoch_score=0\n",
        "        for episode in range(n_episodes):\n",
        "            action = agent.act(state,epsilon)\n",
        "            next_state,reward,done,info = env.step(action)\n",
        "            loss = agent.step(state,action,reward,next_state,done)\n",
        "            \n",
        "            if agent.train_flag:\n",
        "                losses.append(loss)\n",
        "            \n",
        "            state =next_state\n",
        "            epoch_score+=reward\n",
        "\n",
        "            #epsilon_decay\n",
        "            if epsilon>eps_end:\n",
        "                epsilon*=eps_decay\n",
        "\n",
        "            if done:\n",
        "                scores.append(epoch_score)\n",
        "                break\n",
        "\n",
        "        if epoch %100==0 and epoch>0:\n",
        "            \n",
        "            mean_loss = np.sum(losses)/len(losses)\n",
        "            mean_reward = np.sum(scores)/len(scores)\n",
        "            losses=[]\n",
        "            scores=[]            \n",
        "            print('epoch: {0}, mean_loss: {1},mean_reward:{2}, memory_size: {3}, epsilon: {4}'.format(epoch,mean_loss,mean_reward,agent.memory.size(),epsilon))\n",
        "        scores.append(epoch_score)\n",
        "\n",
        "    return scores,losses\n",
        "\n",
        "scores,losses = training()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 100, mean_loss: 0.05511687323451042,mean_reward:147.609756097561, memory_size: 10001, epsilon: 0.009999536012924389\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-68f0f9763505>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-68f0f9763505>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(n_episodes, max_t, eps_start, eps_end, eps_decay)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mmean_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mmean_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mscores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-BfmDYTgVd0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(scores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMHYuW6XY3kY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}