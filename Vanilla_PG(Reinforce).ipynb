{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Policy_gradient.ipynb","provenance":[],"authorship_tag":"ABX9TyNDNsGY46dRDsrLiio1KD3L"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"ZTOm8b7OnAnI","colab_type":"code","colab":{}},"source":["import torch\n","import gym\n","\n","import numpy as np  \n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","# import torch.distributions.categorical\n","from torch.distributions.categorical import Categorical\n","\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bkMUQdixksgq","colab_type":"code","outputId":"90532ff6-2c41-4660-9927-484b8b9f6ef0","executionInfo":{"status":"ok","timestamp":1584025119191,"user_tz":-180,"elapsed":1153,"user":{"displayName":"Рамиль","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghg91A7ku2EOFBsVNl3HeuDpI3E6uk8XPMLels5=s64","userId":"09925931107143194545"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["env = gym.make('CartPole-v1')\n","\n","state_size = env.observation_space.shape[0]\n","action_size  =env.action_space.n\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n","  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"S1ujtBR6uPlj","colab_type":"code","colab":{}},"source":["GAMMA=0.99\n","\n","class PolicyNetwork(nn.Module):\n","    def __init__(self,state_size,action_size):\n","        super(PolicyNetwork, self).__init__()\n","\n","        self.layers = nn.Sequential(\n","            nn.Linear(state_size,128),\n","            nn.Dropout(p=0.6),\n","            nn.ReLU(),\n","            nn.Linear(128,action_size),\n","            nn.Softmax(), \n","\n","\n","        )\n","    def forward(self,x):\n","        x = torch.FloatTensor(x)\n","        output = self.layers(x)\n","        \n","        return output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pEDcFuXUq-MD","colab_type":"code","colab":{}},"source":["class ReinforceAgent:\n","    def __init__(self,state_size,action_size):\n","        self.policy_net = PolicyNetwork(state_size,action_size)\n","        self.optimizer = optim.Adam(self.policy_net.parameters(),lr=1e-2)\n","\n","        self.state_size = state_size\n","        self.action_size = action_size\n","        \n","\n","    #this should be ok\n","    def get_action(self,state):\n","        state = torch.FloatTensor(state)\n","        #this will return probabilities from which we will sample\n","        probs = self.policy_net.forward(state) \n","        # print(probs)\n","\n","        #one way of sampling using numpy\n","        # probs_np = log_probs.detach().numpy()\n","        # action = np.random.choice(range(action_size),p = probs_np)\n","\n","        #second way of sampling using pytorch(logits will be converted into probs)\n","        sampler = Categorical(probs=probs)\n","        action = sampler.sample()\n","        \n","        log_prob = sampler.log_prob(action)\n","        # print(log_prob)\n","\n","        return action.item(),log_prob\n","\n","    #here i update policy_network \n","    def update_policy(self,log_probs,rewards): #states and actions should be torch.FloatTensor\n","        #compute logits(outputs from NN)\n","        # logits = self.policy_net.forward(states)\n","        #make a Categorical to be able to sample\n","        # sampler = Categorical(logits)\n","        #compute logP(A|state)\n","        # logp = sampler.log_prob(actions)\n","\n","        #compute mean loss for the whole episode\n","\n","        rewards = discount(rewards)\n","        loss = []\n","        for logp,reward in zip(log_probs,rewards):\n","            loss.append(-logp*reward)\n","        \n","        loss = torch.stack(loss)\n","\n","        # we should use mean loss, but it doesn't work and i don't know why\n","        # loss = loss.mean()\n","        loss = loss.sum()\n","        \n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()\n","\n","def discount(rewards,GAMMA=1):\n","    #this is like dynamic programming\n","    R = 0\n","    discounted_rewards=rewards.copy()\n","    #discounting rewards \n","    for r in rewards[::-1]:\n","        R = r + GAMMA*R\n","        #it appends to the beginning of the list, first value is the index of element before which we want to append\n","        discounted_rewards.insert(0,R)\n","\n","    discounted_rewards = torch.tensor(discounted_rewards)\n","\n","    #normalizing (x-mean(x))/std\n","    discounted_rewards = (discounted_rewards - discounted_rewards.mean())/(discounted_rewards.std() + 1e-9)\n","\n","    return discounted_rewards\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"T3uQ4aG_nIkO","colab_type":"code","colab":{}},"source":["agent = ReinforceAgent(state_size,action_size)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sV58GjAJoZMa","colab_type":"code","outputId":"3acc0246-dc20-4d6b-a0e5-c4c0d86d23fc","executionInfo":{"status":"ok","timestamp":1584025119200,"user_tz":-180,"elapsed":1128,"user":{"displayName":"Рамиль","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghg91A7ku2EOFBsVNl3HeuDpI3E6uk8XPMLels5=s64","userId":"09925931107143194545"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["max_episode_num = 15000\n","max_steps = 10000\n","\n","def training():\n","    avg_rewards = []\n","    #main loop aka epochs\n","    for episode in range(1):\n","        state = env.reset()\n","\n","        rewards=[]\n","        weights=[]\n","        actions=[]\n","\n","        \n","        log_probs=[]\n","        while True:\n","\n","            action,log_prob = agent.get_action(state)\n","            \n","            actions.append(action)\n","            log_probs.append(log_prob)\n","            \n","            new_state,reward, done, _ = env.step(action)\n","            \n","            state = new_state\n","            rewards.append(reward)\n","            \n","            if done:\n","\n","                log_probs = torch.stack(log_probs)\n","\n","\n","                agent.update_policy(log_probs,rewards)\n","                avg_rewards.append(sum(rewards))\n","\n","                if episode % 500 == 0:\n","                    avg_rew = sum(avg_rewards)/len(avg_rewards)\n","                    print(\"episode: {}, episode reward: {}, average reward: {}\".format(episode, sum(rewards), avg_rew))\n","\n","                \n","                break\n","\n","            \n","training()\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["episode: 0, episode reward: 22.0, average reward: 22.0\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:100: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"gqPwaMXO9V-Q","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}